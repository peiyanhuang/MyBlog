---
layout: post
title:  初识 WebGL
date:   2019-05-01 19:00:00 +0800
categories: 开发者
tag: JS
---

* content
{:toc}

### 1.什么是WebGL

WebGL 是一个绘图接口: WebGL 提供了 JavaScript 编程接口，基于 HTML5 Canvas 元素实现 2D 和 3D 绘图，我们通过 Canvas 来获取 WebGL 特定的绘制上下文。

WebGL 基于 OpenGL ES 2.0: OpenGL ES 是三维绘图标准OpenGL针对嵌入式系统的一个定制版本。ES 表示 Embbed System，这意味着它被用于小型计算设备，特别是手机和平板电脑。通过提供精简化的 OpenGL，一致的、跨平台、跨浏览器的3D API更容易被实现。(而另一种程序接口系统是仅用于Microsoft Windows上的Direct3D)

WebGL 是一个底层的技术，更加接近硬件，因此它效率很高，但麻烦的是它除了 JS 编程接口外，还移植了 OpenGL 着色语言（GLSL），GLSL 是基于 C 的系统级编程语言，用来开发显卡应用程序，我们需要对此有一定的了解。

WebGL可和其他页面内容共存: 因为其依托于 Canvas 元素，所以它将占用页面的一个区域，可以包含在一个 div标签内部。WebGL 的图形和其他 HTML 元素独立绘制，由浏览器来负责合成并最终呈现给用户。

尽管WebGL支持2D绘图，但其主要用途是用来建构3D场景。

[兼容性](https://caniuse.com/#search=webgl)

### 2.WebGL绘制流程

WebGL 经常被当成 3D API，事实上 WebGL 仅仅是一个光栅化引擎，它可以根据你的代码绘制出点，线和三角形。 

WebGL 程序由 Javascript 的控制代码，和在计算机的图形处理单元（GPU, Graphics Processing Unit）中执行的特效代码(shader code，渲染代码) 组成。

WebGL 直接基于 GPU 来绘制图像，GPU 利用专有的图形内存/缓存来加速渲染，帧缓存包含颜色、透明度、深度和模版掩码属性。GPU把像素写入帧缓存，显示设备从缓存中进行光栅化扫描读取，为了保持读写同步，GPU使用了双缓存（前缓存和后缓存）以及垂直同步（vSync）技术，所谓vSync就是在光栅扫描结束时给GPU发出信号，使得GPU可以置换前后缓存。

**光栅化**（Rasterization）是把顶点数据转换为片元的过程，具有将图转化为一个个栅格组成的图象的作用，特点是每个元素对应帧缓冲区中的一像素。光栅化这个词儿Adobe官方翻译成栅格化或者像素化。就是把矢量图形转化成像素点的过程。我们屏幕上显示的画面都是由像素组成，而三维物体都是点线面构成的。要让点线面，变成能在屏幕上显示的像素，就需要 Rasterize 这个过程。就是从矢量的点线面的描述，变成像素的描述。

注：特效代码使用一种和C或C++类似的强类型的语言 GLSL(GL着色语言)。这样的代码也被称为着色器，WebGL 每次绘制需要两个着色器，一个**顶点着色器**和一个**片断着色器**。

![image]({{ '/images/webgl-1.png'|prepend:site.baseurl}})

工作流水线指的是WebGL程序的执行过程，如上图所示，主要分为4个步骤，

- 第1步是顶点着色器的处理，主要是一组矩阵变换操作，用来把3D模型（顶点和原型）投影到视图(Viewport)上，输出是一个个多边形（比如三角形）；
- 第2步是光栅化，也就是把三角形连接区域按一定的粒度逐行转化成片段（fragement），我们可以把这些片段看做是3D空间的像素点；
- 第3步是片段着色器的处理，为每个像素添加颜色；
- 第4步把3D空间的片段合并输出为2D像素数组并显示在屏幕上。

#### 2.1顶点着色器

顶点(Vertex)不只是立体几何中的坐标点，它还包含颜色、纹理，以及表面法向量（通常是由表面向外）用来区分对象的正反面。表面法向量遵循右手规则（或逆时针旋转）。

为了得到一个真实的3D场景，仅仅绘制某些位置的对象是不够的，还需要考虑到灯光照射到这些对象时的效果。用一个通用的术语表示不同材质在不同条件下的展示，即着色（shading）。

顶点着色实际上并不是单纯改变顶点颜色的意思，还包括把每个对象放置在场景中的某个位置上。可当做是对顶点的处理。

#### 2.2图元装配

图元(Primitives)是一组最简单的图形元素，如点、线段和若干点的闭合连接。

把已经着色的顶点装配成三角形，线段或点等几何图元。判断它们在当前时刻是否处于视锥体可视区内，并删除区域外的图元。

![image]({{'/images/tuyuan.png'|prepend:site.baseurl}})

#### 2.3光栅化

将图元转换为片段，然后把片段传送给片段着色器。

我们可以把片段看成最终绘制在屏幕的一个像素

#### 2.4片段着色器

片段是三维空间的数据，坐标为（x,y,z），x/y和2D像素相同，z表示颜色深度，合成时需要处理前后遮挡关系以及透明度的处理。除此之外，片段是顶点光栅化而来，其还包含和顶点(Vertex)对象相同的属性，比如纹理。

片断着色器的作用是计算出当前绘制图元中每个像素的颜色值。每一对顶点着色器和片段着色器组合起来称作一个 program（着色程序）。

现代GPU中，顶点处理和片段处理都是可编程的，称之为着色器（Shader）。顶点处理包括如下变换：

1. 放置对象在3D空间中，也就是模型变换（Model Transformation）
2. 设定相机的位置和观察方向，称之为视图变换或取景变换（View transformation）
3. 选择相机镜头（广角，正常或可伸缩的），调整焦距和缩放系数来设定相机的视觉空间，称之为投影变换(Projection transformation)，投影方式有两种：正交投影和远景投影，其中常用的是远景投影，即模拟人眼来观察真实世界中的对象，正交投影可以理解为把观察点拉到足够远，此时视觉空间将逼近为平行的长方体而不再是一个视椎体。
4. 把相片打印到选定纸面区域，称之为视窗变换（Viewport transformation），这一步是在光栅化的时候执行的，注意Viewport实际对应于屏幕Web应用程序的窗口，其坐标原点是在左上角，y轴的方向向下。

### 3.GL数据传递

1. 属性（Attributes）和缓冲

缓冲是发送到GPU的一些二进制数据序列，通常情况下缓冲数据包括位置，法向量，纹理坐标，顶点颜色值等。 你可以存储任何数据。

属性用来指明怎么从缓冲中获取所需数据并将它提供给顶点着色器。 例如你可能在缓冲中用三个32位的浮点型数据存储一个位置值。 对于一个确切的属性你需要告诉它从哪个缓冲中获取数据，获取什么类型的数据（三个32位的浮点数据）， 起始偏移值是多少，到下一个位置的字节数是多少。

缓冲不是随意读取的。事实上顶点着色器运行的次数是一个指定的确切数字， 每一次运行属性会从指定的缓冲中按照指定规则依次获取下一个值。

2. 全局变量（Uniforms）

全局变量在着色程序运行前赋值，在运行过程中全局有效。

3. 纹理（Textures）

纹理是一个数据序列，可以在着色程序运行中随意读取其中的数据。 大多数情况存放的是图像数据，但是纹理仅仅是数据序列， 你也可以随意存放除了颜色数据以外的其它数据。

4. 可变量（Varyings）

可变量是一种顶点着色器给片断着色器传值的方式，依照渲染的图元是点， 线还是三角形，顶点着色器中设置的可变量会在片断着色器运行中获取不同的插值。

### 4.坐标系

a. 物体坐标系

物体坐标系是和特定物体相关联的坐标系。每个物体都有它们的独立坐标系。例如：每个人都带着自己的坐标系。如果我告诉自己向前走一步，是在向我的物体坐标系来发指令。这个方向是对于我而言的绝对方向，我的左对应的可能是你的右。所以，前后左右这样的指令是仅仅相对于物体坐标系。

当一个人告诉我向左的时候，他心里想的是物体坐标系；当一个人告诉我向东的时候，他心里想的是世界坐标系。

b. 世界坐标系

世界坐标系是一个特殊的坐标系，它建立了描述其他坐标系所需要的参考框架。 换句话说，能够用世界坐标系描述其他坐标系的位置，而不能用更大的、 外部坐标系来描述世界坐标系。

c. 摄像机坐标系、屏幕坐标系

摄像机坐标系和屏幕坐标系类似，差别在于摄像机坐标系处于3D空间，而屏幕坐标系在2D平面里。摄像机坐标系可以被看作是一种特殊的物体坐标系。

d. 惯性坐标系

为了简化世界坐标系到物体坐标系的转换，人们引入了一种新的坐标系，叫做惯性坐标系。惯性坐标系的原点和物体坐标系的原点重合，但是轴平行于世界坐标系。

引入惯性坐标系的原因：物体坐标系到惯性坐标系的转换只需要旋转，惯性坐标系到世界坐标系只需要平移，相当于把一件事拆成了两件。

- 变换过程

物体坐标(模型变换) → 世界坐标(视图变换) → 摄像机坐标(投影变换) → CCV坐标 → 屏幕坐标

屏幕坐标是以设备屏幕的左上角为原点，宽为x轴，高为z轴的坐标系。

这个过程也就是3D和2D坐标的相互转换。

### 5.二维变换--平移、旋转、缩放

- 平移

```gl
attribute vec2 a_position;
 
uniform vec2 u_resolution;
uniform vec2 u_translation;
 
void main() {
  // 加上平移量
  vec2 position = a_position + u_translation;
  
  vec2 zeroToOne = position / u_resolution;
  
  vec2 zeroToTwo = zeroToOne * 2.0;
  vec2 clipSpace = zeroToTwo - 1.0;
  gl_Position = vec4(clipSpace, 0, 1);
}
```

- 旋转

x1=cos(angle)*x+sin(angle)*y;

y1=cos(angle)*y-sin(angle)*x;

```gl
attribute vec2 a_position;
 
uniform vec2 u_resolution;
uniform vec2 u_translation;
uniform vec2 u_rotation;
 
void main() {
  // 旋转位置
  vec2 rotatedPosition = vec2(
    a_position.x * u_rotation.y + a_position.y * u_rotation.x,
    a_position.y * u_rotation.y - a_position.x * u_rotation.x
  );
  
  // 加上平移
  vec2 position = rotatedPosition + u_translation;
}
```

- 缩放

```gl
attribute vec2 a_position;

uniform vec2 u_scale;

oid main() {
  vec2 scaledPosition = a_position * u_scale;
}
```

- 矩阵变换

```js
const m3 = {
  translation (tx, ty) {
    return [
      1, 0, 0,
      0, 1, 0,
      tx, ty, 1,
    ];
  },
  rotation (angleInRadians) {
    const c = Math.cos(angleInRadians);
    const s = Math.sin(angleInRadians);
    return [
      c,-s, 0,
      s, c, 0,
      0, 0, 1,
    ];
  },
  scaling (sx, sy) {
    return [
      sx, 0, 0,
      0, sy, 0,
      0, 0, 1,
    ];
  },
};

// GL
uniform mat3 u_matrix;
vec2 position = (u_matrix * vec3(a_position, 1)).xy;
```

### 6.三维

- 相机（Cameras）, 视角（Perspective）, 视窗（Viewports）

Camera 指的是3D系统所使用的观察点，在这个点来拍摄整个场景。Perspective指的是观察空间，Viewports指的是对象在可视窗口的投影。

有时候我们通过移动相机的位置来获得拍摄特效，这些通过矩阵变换来实现。Cameras 定义了一组矩阵，第1个矩阵定义相机的位置和方向，第2个矩阵定义了到 viewport 的位移，被称之为投影矩阵（projection matrix）。

下图对 Camera 拍摄空间的这些核心概念有一个直观的描述：

![image]({{ '/images/webgl-2.png'|prepend:site.baseurl}})

三维投影分为正交投影和透视投影。区别在于透视投影具有近大远小的特征。

WebGL中的三角形有正反面的概念，正面三角形的顶点顺序是逆时针方向，反面三角形是顺时针方向。

一个三角形是顺时针还是逆时针是根据裁剪空间中的顶点顺序判断的，换句话说，WebGL是根据你在顶点着色器中运算后提供的结果来判定的，这就意味着如果你把一个顺时针的三角形沿 X 轴缩放 -1，它将会变成逆时针，或者将顺时针的三角形旋转180度后变成逆时针。

```js
gl.enable(gl.CULL_FACE); // 激活后默认不绘制背面 gl.cullFace(mode);
```

- DEPTH BUFFER(深度缓冲)：

深度缓冲有时也叫 `Z-Buffer`，是一个存储像素深度的矩形，一个深度像素对应一个着色像素，在绘制图像时组合使用。 当 WebGL 绘制每个着色像素时也会写入深度像素，它的值基于顶点着色器返回的 Z 值，就像我们将 X 和 Y 转换到裁剪空间一样(-1 到 +1) 。Z 这个值会被转换到深度空间( 0 到 +1)，WebGL绘制一个着色像素之前会检查对应的深度像素，如果对应的深度像素中的深度值小于当前像素的深度值，WebGL就不会绘制新的颜色。 反之它会绘制片断着色器提供的新颜色并更新深度像素中的深度值。 这也意味着在其他像素后面的像素不会被绘制。

我们可以像这样开启这个特性

```js
gl.enable(gl.DEPTH_TEST);
```

### 7.纹理

纹理是通过“纹理坐标”来引用的，纹理坐标 0.0 到 1.0 对应纹理从左到右，0.0 到 1.0 对应第一个像素所在行到最后一行。 注意没有使用上或者下，上下在纹理坐标空间中是没有意义的，因为绘制一些东西后再重定向后，是没有上下的概念的，主要是依据传递给WebGL的纹理数据，纹理数据的开头对应纹理坐标 (0, 0)，结尾对应纹理坐标 (1, 1)。

绘制纹理的基本原理是，为每个顶点指定一个纹理坐标(在(0,0)与(1,1)的正方形中)，然后传入纹理对象。片元着色器拿到的是对应片元的内插后的纹理坐标，就利用这个纹理坐标去纹理对象上取颜色，再画到片元上。

内插后的纹理坐标很可能不恰好对应纹理上的某个像素，而是在几个像素之间(因为通常的图片纹理也是离散)。
假设要把一张 16×16 大小的纹理绘制到屏幕上 2×2 大小的区域。那么这 4 个像素应该使用什么颜色？这里有 256 个像素可以选择，如果在 Photoshop 中将 16×16 的图像缩放到 2×2，它会将每个角 8×8 的像素的平均值赋给这四个像素。但是绘制 64 个像素再求平均在 GPU 中是非常慢的。假设有一个 2048x2084 像素的纹理想要绘制成 2x2 个像素，就需要对 1024x1024 或 100 万个像素求平均 4 次，这需要很多运算同时速度要快。

事实上 GPU 使用的是一个纹理贴图(mipmap)，纹理贴图是一个逐渐缩小的图像集合，每一个是前一个的四分之一大小。这就是 `gl.generateMipmap` 做的事情，它根据原始图像创建所有的缩小级别，也可以自己提供缩小级别的图像。

### 8.混合与蒙版

透明效果是用混合机制完成的。混合机制与深度检测类似，也发生在试图向某个已填充的像素填充颜色时。深度检测通过比较z值来确定像素的颜色，而混合机制会将两种颜色混合。

混合的顺序是按照绘制的顺序进行的，如果绘制的顺序有变化，混合的结果通常也不同。如果模型既有非透明表面又有透明表面，绘制透明表面时开启蒙版，其目的是锁定深度缓冲区，因为半透明物体后面的物体还是可以看到的，如果不这样做，半透明物体后面的物体将会被深度检测机制排除。

开启混合的代码如下。`gl.blendFunc` 方法指定了混合的方式，这里的意思是，使用源(待混合)颜色的 α 值乘以源颜色，加上 1-[源颜色的 α]乘以目标颜色。

```js
gl.enable(gl.BLEND);
gl.blendFunc(gl.SRC_ALPHA, gl.ONE_MINUS_SRC_ALPHA);
```

### 9.WebGL2

WebGL2 几乎 100% 向后兼容 WebGL1. 如果只是使用 WebGL1 的特性，只有两点不同。

1. 调用 `getContext` 时，使用 `webgl2` 代替 `webgl`;
2. 许多扩展成为 `WebGL2` 的标准部分，所以不再用扩展的方式获得。例如，顶点数组对象 `OES_vertex_array_object` 是 WebGL2 标准特性。例如使用 WebGL1 是这样做的:

```js
const ext = gl.getExtension("OES_vertex_array_object");
if (!ext) {
  // 告诉用户没有此扩展或者使用其他方式
} else {
  const someVAO = ext.createVertexArrayOES();
}
```

使用 WebGL2 可以直接应该这样写:

```js
const someVAO = gl.createVertexArray();
```

- 切换到 GLSL 300 es

最大的变化是你应该将着色器升级到GLSL 3.00 ES。着色器第一行需要是

```GLSL
#version 300 es
```

注意：必须严格在第一行。不能有注释，不能有空行，空格。

- 在着色器中要改动的差异

```bash
attribute -> in
varying -> in / out (片段/顶点)
```

-支持非2的幂纹理：在 WebGL1 中，不是 2 的幂大小的纹理不能有 mip。 在 WebGL2 中，限制被去掉了。不是 2 的幂大小的纹理同样工作。

[具体区别](https://webgl2fundamentals.org/webgl/lessons/zh_cn/webgl1-to-webgl2.html)

### 10. Three.js

对于 WebGL 来说，我认为他只是光栅化 API，因为你想绘制一些三维物体时，需要大量三维数学知识。

我认为称作是三维库的东西应该帮你完成三维部分，你应该可以给它提供一些三维数据， 一些材质参数，一些光源它就会帮你画出三维场景。WebGL (和 OpenGL ES 2.0+) 都被用来绘制三维但都不符合这个描述。

举一个例子，C++ 本身不会提供“处理文字”的功能，即使文字处理器可以用 C++ 写出来但是并不会将 C++ 称为一个“文字处理器”。同样的 WebGL 本身并不能绘制出三维图形，你可以写一个库帮你用 WebGL 绘制三维图形，但 WebGL 本身并不绘制三维图形。

![image]({{ '/images/threejs_components.jpg'|prepend:site.baseurl}})

Three.js 提供了 22 中几何体生成，分别是：

- planeGeometry（平面几何体）
- BoxGeometry（长方体）
- CircleGeometry（圆形）	
- ConeGeometry（圆锥体）
- CylinderGeometry（圆柱体）
- DodecahedronGeometry（十二面体）
- IcosahedronGeometry（二十面体）
-	LatheGeometry（让任意曲线绕 y 轴旋转生成一个形状，如花瓶）
- OctahedronGeometry（八面体）
- ParametricGeometry（根据参数生成形状）
- PolyhedronGeometry（多面体）	
- RingGeometry（环形）	
- ShapeGeometry（二维形状）
- SphereGeometry（球体）	
- TetrahedronGeometry（四面体）
- TorusGeometry（圆环体）
- TorusKnotGeometry（换面纽结体）
- TubeGeometry（管道）

- ExtrudeGeometry: 则是按照指定参数将一个二维图形沿 z 轴拉伸出一个三维图形
- TextGeometry: 则需要从外部加载特定格式的字体文件（可在 [typeface.js 网站](https://gero3.github.io/facetype.js/)上进行转换）进行渲染，其内部依然使用 ExtrudeGeometry 对字体进行拉伸，从而形成三维字体。另外，该类字体的本质是一系列类似 SVG 的指令。所以，字体越简单（如直线越多），就越容易被正确渲染。
- EdgesGeometry: 边缘几何体
- WireframeGeometry: 网格几何体

